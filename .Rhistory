n <- 10
xbars_star < rep(NA, B)
xbars_star[b] <- mean(sample(c(0,1), size = n, replace = T))
for(b in 1:B) {
xbars_star[b] <- mean(sample(c(0,1), size = n, replace = T))
}
hist(xbars_star, nclass= 100, main = 'Sample mean of 10 obs')
library(ggplot2)
library(dplyr)
mpg
glimpse(mpg)
head(mpg)
summary(mpg)
summary(mpg$hwy)
mean(mpg$hwy)
range(mpg$hwy)
quantile(mpg$hwy)
oper <- par(mfrow=c(2,2))
hist(mpg$hwy)
boxplot(mpg$hwy)
qqnorm(mpg$hwy)
qqline(mpg$hwy)
opar <- par(mfrow=c(2,2))
hist(mpg$hwy)
boxplot(mpg$hwy)
qqnorm(mpg$hwy)
qqline(mpg$hwy)
par()
par(par)
par(opar)
source('~/ipds_kr_R_book/ch05_statistic_concepts/ch07_basic_analysis.R', echo=TRUE)
hwy < mpg$hwy
hwy <- mpg$hwy
n <- length(hwy)
mu0 <- 22.9
t.test(hwy, mu = mu0, alternative = 'greater')
t.test(hwy)
c(mean(hwy), sd(hwy))
c(median(hwy), mad(hwy))
set.seed(2019)
n <- 100
p <- .5
x <- binom(n, 1, p)
x <- rbinom(n, 1, p)
x <- factor(x, levels = c(0,1), labels = c('no', 'yes'))
x
table(x)
prop.table(table(x))
barplot(table(x))
par(opar)
barplot(table(x))
par(mfrow = c(1,1))
table(x)
prop.table(table(x))
barplot(table(x))
binom.test(x=length(x[x=='yes']), n=length(x), p=.5, alternative = "two.sided")
binom.test(x=5400, n=10000)
n <- c(100, 1000, 2000, 10000, 1e6)
data.frame(n=n, moe=round(1.96*sqrt(1/(4*n)), 4))
curve(1.96 * sqrt(1/(4*x), 10, 10000, log = 'x'))
curve(1.96 * sqrt(1/(4*x)), 10, 10000, log = 'x')
ggplot(mpg, aes(cty, hwy)) + geom_jitter() + geom_smooth(method = "lm")
with(mpg, cor(cty, hwy))
with(mpg, cor(cty, hwy, method  "kendall"))
with(mpg, cor(cty, hwy, method="kendall"))
with(mpg, cor(cty, hwy, method="kendall"))
with(mpg, cor(cty, hwy, method="spearman"))
hwy_lm <- lm(hwy ~ cty, data = mpg)
summary(hwy_lm)
predict(hwy_lm)
predict(hwy_lm, newdata = data.frame(cty = c(10, 20, 30)))
predict(hwy_lm, newdata = data.frame(cty = c(10, 20, 30)),
se.fit = T)
class(hwy_lm)
opar <- par(mfrow = c(2,2), oma = c(0, 0, 1.1, 0))
plot(hwy_lm, las=1)
library(MASS)
set.seed(123)
lqs(stack.loss ~ ., data = stackloss)
lm(stack.loss ~., data = stackloss)
plot(hwy ~ displ, data = mpg)
par(mfrow = c(1, 1))
plot(hwy ~ displ, data = mpg)
mpg_lo <- loess(hwy ~ displ, data = mpg)
mpg_lo
summary(mpg_lo)
ggplot(mpg, aes(displ, hwy)) + geom_point() +
geom_smooth()
mpg %>% ggplot(aes(class, hwy)) + geom_boxplot()
hwy_lm2 <- lm(hwy ~ class, data = mpg)
summary(hwy_lm2)
predict(hwy_lm2, newdata = data.frame(class-"pickup"))
predict(hwy_lm2, newdata = data.frame(class="pickup"))
opar <- par(mfrow = c(2,2), oma = c(0,0,1.1,0))
plot(hwy_lm2, las=1)
source('~/ipds_kr_R_book/ch05_statistic_concepts/ch07_basic_analysis.R')
read.csv('https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/challenger.csv')
chall <- read.csv('https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/challenger.csv')
chall %>% tbl_df() -> chall
chall
glimpse(chal)
glimpse(chall)
chall %>% ggplot(aes(temperature, distress_ct)) + geom_point()
chall %>% ggplot(aes(factor(distress_ct), temperature))
chall %>% ggplot(aes(factor(distress_ct), temperature)) + geom_boxplot()
chall_glm <-
glm(cbind(distress_ct, o_ring_ct - distress_ct) ~
temperature, data = chall, family = 'binomial')
chall_glm
chall_glm
summary(chall_glm)
predict(chall_glm, data.frame(temperature = 30))
exp(3.45) / (exp(3.45) + 1)
predict(chall_glm, data.frame(temperature = 30), type = 'response')
logistic <- funcion(x) {exp(x) / (exp(x) + 1)}
logistic <- funcion(x) {exp(x) / (exp(x) + 1)}
logistic <- funcion(x) {
exp(x) / (exp(x) + 1)
}
x
logistic <- funcion(x) {exp(x)/(exp(x) + 1)}
logistic <- function(x) {exp(x)/(exp(x) + 1)}
logistic
plot(c(20, 85), c(0, 1), type = "n", xlab = "temperature")
logistic <- function(x){exp(x)/(exp(x)+1)}
plot(c(20,85), c(0,1), type = "n", xlab = "temperature",
ylab = "prob")
tp <- seq(20, 85, 1)
chall_glm_pred <-
predict(chall_glm,
data.frame(temperature = tp),
se.fit = TRUE)
lines(tp, logistic(chall_glm_pred$fit))
lines(tp, logistic(chall_glm_pred$fit - 1.96 * chall_glm_pred$se.fit), lty=2)
lines(tp, logistic(chall_glm_pred$fit + 1.96 * chall_glm_pred$se.fit), lty=2)
abline(v=30, lty=2, col='blue')
par(mfrow =c(1,1))
logistic <- function(x){exp(x)/(exp(x)+1)}
plot(c(20,85), c(0,1), type = "n", xlab = "temperature",
ylab = "prob")
tp <- seq(20, 85, 1)
chall_glm_pred <-
predict(chall_glm,
data.frame(temperature = tp),
se.fit = TRUE)
lines(tp, logistic(chall_glm_pred$fit))
lines(tp, logistic(chall_glm_pred$fit - 1.96 * chall_glm_pred$se.fit), lty=2)
lines(tp, logistic(chall_glm_pred$fit + 1.96 * chall_glm_pred$se.fit), lty=2)
abline(v=30, lty=2, col='blue')
chall %>% write_csv("chall.csv")
Packages <- c('tidyverse', 'data.table', 'reshape2', 'caret', 'rpart', 'GGally',
'ROCR', 'ISLR','glmnet', 'gbm', 'boot', 'randomForest', 'dummies',
'curl', 'gridExtra')
lapply(Packages, library, character.only=T)
adult <- read.csv("adult.data", header = F, strip.white = T)
setwd('C:/Users/Daniel/Documents/ipds_kr_R_book/ch08_classification/')
adult <- read.csv("adult.data", header = F, strip.white = T)
setwd('C:/Users/Daniel/Documents/ipds_kr_R_book/ch08_classification/adult')
adult <- read.csv("adult.data", header = F, strip.white = T)
adult
names(adult) <- c("age", "workclass", "fnlwgt", "education", "education_num",
"marital_status", "occupation", "relationship", "race", "sex",
"capital_gain", "capital_loss", "hours_per_week", "native_country",
"wage")
adult %>% head()
glimpse(adult)
summary(adult)
levels(adult$wage)
adult$race
adult$race %>% distinct()
adult$race %>% as.character() %>% disticnt()
adult$race %>% as.character() %>% unique()
glimpse(adult)
model.matrix(~ race + sex + age, adult)
x <- model.matrix(~ race + sex + age, adult)
glimpse(x)
x <- model.matrix(~ race + sex + age, adult)
glimpse(x)
x
glimpse(x)
glimpse(x)
class(x)
x %>% as.data.frame()
x %>% as.data.frame() %>% as_tibble()
x <- x %>% as.data.frame() %>% as_tibble()
x %>% glimpse()
colnames(x)
model.matrix(~ . -wage, adult )
model.matrix(~ . -wage, adult ) %>% as.data.frame() %>% as_tibble()
model.matrix(~ . -wage, adult ) %>% as.data.frame() %>% as_tibble() -> x
dim(x)
set.seed(2019)
n <- nrow(adudlt)
n <- nrow(adult)
n
idx <- 1:n
training_idx <- sample(idx, n * .6)
training_idx
idx <- setdiff(idx, training_idx)
validate_idx <- sample(idx, n * .5)
validate_idx <- sample(idx, n * .2)
test_idx <- setdiff(idx, validate_idx)
test_idx
length(training_idx)
length(validate_idx)
length(test_idx)
traing <- adult[traing_idx, ]
traing <- adult[training_idx, ]
validation <- adult[validate_idx, ]
test <- adult[test_idx, ]
training %>%
ggplot(aes(age)) %>% geom_density(alpha=.5)
training <- adult[training_idx, ]
validation <- adult[validate_idx, ]
test <- adult[test_idx, ]
training %>%
ggplot(aes(age)) %>% geom_density(alpha=.5)
training %>% ggplot(aes(age)) %>% geom_density(alpha=.5)
training %>% ggplot(aes(age)) + geom_density(alpha=.5)
training %>% ggplot(aes(age, fill = wage)) + geom_density(alpha=.5)
training %>%
filter(race %in% c('Black', 'White')) %>%
ggplot(aes(age, fill = wage)) +
geom_density(alpha = .5) +
ylim(0, .1) +
facet_grid(race ~ sex, scales = 'free_y')
training %>%
ggplot(aes(education_num, fill = wage)) %>%
geom_bar()
training %>%
ggplot(aes(education_num, fill = wage)) +
geom_bar()
ad_glm_full <- glm(wage ~ ., data = training, family = binomial)
ad_glm_full
summary(ad_glm_full)
alias(ad_glm_full)
summary.glm()
predict(ad_glm_full, newdata = adult[1:5, ], type = "response")
# at least 5 times tials of data visualization needed...
training$wage
y_obs <- ifelse(validation$wage == ">50K", 1, 0)
y_obs
yhat_lm <- predict(ad_glm_full, newdata = validation, type = 'response')
yhat_lm
data.frame(y_obs, yhat_lm)
result_with_training <- data.frame(y_obs, yhat_lm)
result_with_training
result_with_training %>%
ggplot(aes(y_obs, yhat_lm, group = y_obs, fill = factor(y_obs)))
result_with_training %>%
ggplot(aes(y_obs, yhat_lm, group = y_obs, fill = factor(y_obs))) +
geom_boxplot()
result_with_training %>%
ggplot(aes(y_obs, yhat_lm, group = y_obs, fill = factor(y_obs))) +
geom_boxplot() -> p1
result_with_training %>%
ggplot(aes(yhat_lm, fill=factor(y_obs))) +
geom_density(alpha = .5)
result_with_training %>%
ggplot(aes(yhat_lm, fill=factor(y_obs))) +
geom_density(alpha = .5) -> p2
library(gridExtra)
grid.arrange(p1, p2, ncol = 2)
bimomial_deviance <- function(y_obs, yhat) {
epsilon = 0.0001
yhat = ifelse(yhat < epsilon, epsilon, yhat)
yhat = ifelse(yhat > 1- epsilon, 1-epsilon, yhat)
a = ifelse(y_obs == 0, 0, y_obs * log(y_obs/yhat))
b = ifelse(y_obs==1, (1-y_obs) * log((1-y_obs)/(1-yhat)))
return((2*sum(a+b)))
}
library(ROCR)
bimomial_deviance(y_obs, yhat_lm)
b = ifelse(y_obs==1, 0, (1-y_obs) * log((1-y_obs)/(1-yhat)))
bimomial_deviance <- function(y_obs, yhat) {
epsilon = 0.0001
yhat = ifelse(yhat < epsilon, epsilon, yhat)
yhat = ifelse(yhat > 1- epsilon, 1-epsilon, yhat)
a = ifelse(y_obs == 0, 0, y_obs * log(y_obs/yhat))
b = ifelse(y_obs==1, 0, (1-y_obs) * log((1-y_obs)/(1-yhat)))
return((2*sum(a+b)))
}
bimomial_deviance(y_obs, yhat_lm)
pred_lm <- prediction(yhat_lm, y_obs)
pred_lm
perf_lm <- performance(pred_lm, measure = 'tpr', x.measure = 'fpr')
plot(perf_lm, col = 'black', main = 'ROC Curve for GLM')
abline(0, 1)
plot(perf_lm, col = 'black', main = 'ROC Curve for GLM', col = 'red')
plot(perf_lm, col = 'black', main = 'ROC Curve for GLM', colpr = 'red')
plot(perf_lm, col = 'black', main = 'ROC Curve for GLM', color = 'red')
abline(0, 1)
plot(perf_lm, col = 'black', main = 'ROC Curve for GLM', color = 'red')
abline(0, 1)
plot(perf_lm, col = 'black', main = 'ROC Curve for GLM', color = 'red')
abline(0, 1)
performance(pred_lm, "auc")@y.values[[1]]
training
xx <- model.matrix(wage ~.-1, adult)
xx
xx[training_idx, ]
x <- xx[training_idx, ]
x
y <- ifelse(training$wage == ">50K", 1, 0)
y
dim(x)
ad_glmnet_fit <- glmnet(x, y)
plot(ad_glmnet_fit)
ad_glmnet_fit
coef(ad_glmnet_fit, s = c(.1713, .1295))
ad.cvfit <- cv.glmnet(x, y, family = "bimomial")
ad.cvfit <- cv.glmnet(x, y, family = "binomial")
plot(ad.cvfit)
log(ad_cvfit$lambda.min)
log(ad.cvfit$lambda.min)
log(ad.cvfit$lambda.lse)
log(ad.cvfit$lambda.min)
log(ad.cvfit$lambda.lse)
ad.cvfit
log(ad.cvfit$lambda.1se)
length(which(coef(ad.cvfit, s = "lambda.min") > 0))
length(which(coef(ad.cvfit, s = "lambda.1se") > 0))
set.seed(2019)
foldid <- sample(1:10, size = length(y), replace = T)
foldid
cv1 <- cv.glmnet(x, y, foldid = foldid, alpha = 1, family = 'binomial')
cv1
cv.5 <- cv.glmnet(x, y, foldid = foldid, alpha = .5, family = 'binomial')
cv0 <- cv.glmnet(x, y, foldid = foldid, alpha = 0, family = 'binomial')
plot(log(cv1$lambda),
cv1$cvm,
pch=19,
col = 'red',
xlab = 'log(Lambda)',
ylab = cv1$name,
main = "alpha=1.0")
point(log(cv.5$lambda), cv.5$cvm, pch = 19, col = "blue")
points(log(cv.5$lambda), cv.5$cvm, pch = 19, col = "blue")
points(log(cv0$lambda), cv0$cvm, pch = 19, col = "grey")
legend("topleft", legend=c("alpha = 1", "alpha = .5", "alpha = 0"),
pch = 19, col =c('red', 'grey', 'blue'))
par(mfrow=c(2,2))
plot(cv1, main = "Alpha=1.0")
plot(cv.5, main = "Alpha=0.5")
plot(cv0, main = "Alpha=0.0")
plot(log(cv1$lambda),
cv1$cvm,
pch=19,
col = 'red',
xlab = 'log(Lambda)',
ylab = cv1$name,
main = "alpha=1.0")
points(log(cv.5$lambda), cv.5$cvm, pch = 19, col = "blue")
points(log(cv0$lambda), cv0$cvm, pch = 19, col = "grey")
legend("topleft", legend=c("alpha = 1", "alpha = .5", "alpha = 0"),
pch = 19, col =c('red', 'grey', 'blue'))
par(mfrow=c(1,1))
predict(ad.cvfit, s = "lambda.1se", newx = x[1:5, ], type = 'response')
# with validation, to find out predict values...
y_obs <- ifelse(validation$wage == ">50K", 1, 0)
y_obs
yhat_glmnet <- predict(ad.cvfit, s = "lambda.1se", new = xx[validate_idx, ],
type = 'response')
yhat_glmnet
yhat_glmnet %>% pull()
yhat_glmnet %>% as_tibble() %>% pull()
yhat_glmnet %>% as_tibble() %>% pull() -> yhat_glmnet
yhat_glmnet
binomial_deviance(y_obs, yhat_glmnet)
plot(ad.cvfit)
epsilon = 0.0001
binomial_deviance <- function(y_obs, yhat) {
epsilon = 0.0001
yhat = ifelse(yhat < epsilon, epsilon, yhat)
yhat = ifelse(yhat > 1- epsilon, 1-epsilon, yhat)
a = ifelse(y_obs == 0, 0, y_obs * log(y_obs/yhat))
b = ifelse(y_obs==1, 0, (1-y_obs) * log((1-y_obs)/(1-yhat)))
return((2*sum(a+b)))
}
binomial_deviance(y_obs, yhat_glmnet)
pred_glmnet <- prediction(yhat_glmnet, y_obs)
pred_glmnet
perf_glmnet <- performance(pred_glmnet, measure = "tpr", x.measure = "fpr")
perf_glmnet <- performance(pred_glmnet, measure = "tpr", x.measure = "fpr")
pred_glmnet <- prediction(yhat_glmnet, y_obs)
perf_glmnet <- performance(pred_glmnet, measure = "tpr", x.measure = "fpr")
plot(perf_lm, col = 'black', main = 'ROC Curve')
abline(0, 1)
plot(perf_glmnet, col = 'blue', add = T)
legend('bottomright', inset=.1,
legend = c("GLM", "glmnet"),
col = c('black', 'blue'),
lty = 1,
add = T)
legend('bottomright', inset=.1,
legend = c("GLM", "glmnet"),
col = c('black', 'blue'),
lty = 1, lwd = 2)
performance(pred_glmnet, "auc")@y.values[[1]]
cvr_tr <- rpart(wage ~ ., data = training)
cvr_tr
printcp(cvr_tr)
summary(cvr_tr)
opar <- par(mfrow = c(1, 1), xpd = NA)
plot(cvr_tr)
text(cvr_tr, use.n = T)
par(opar)
yhat_tr <- predict(cvr_tr, validation)
yhat_tr
yhat_tr <- yhat_tr[, ">50K"]
yhat_tr
binomial_deviance(y_obs, yhat_tr)
pred_tr <- prediction(yhat_tr, y_obs)
pred_tr
perf_tr <- performance(pred_tr, measure = 'tpr', x.measure = 'fpr')
perf_tr
plot(perf_lm, col = 'black', main = "ROC Curve")
plot(perf_tr, col = 'blue', add = T)
abline(0, 1)
legend('bottomright', insert = .1,
legend = c("GLM", "Tree"),
col = c("black", "blue"),
lty = 1, lwd = 2)
legend('bottomright', inset = .1,
legend = c("GLM", "Tree"),
col = c("black", "blue"),
lty = 1, lwd = 2)
performance(pred_tr, "auc")@y.values[[1]]
set.seed(2019)
ad_rf <- randomForest(wage ~ ., training)
ad_rf
plot(ad_rf)
tmp <- importance(ad_rf)
tmp
tmp %>% class()
tmp %>%
rowid_to_column()
tmp %>%
as.data.frame() %>%
rowid_to_column()
tmp %>%
as.data.frame() %>%
rowid_to_column() %>%
as_tibble()
tmp %>%
as.data.frame() %>%
rowid_to_column() %>%
as_tibble() %>%
arrange(MeanDecreaseGini)
tmp %>%
as.data.frame() %>%
rowid_to_column() %>%
as_tibble() %>%
arrange(desc(MeanDecreaseGini))
tmp %>%
as.data.frame()
tmp %>%
as.data.frame() %>%
mutate(var = rowname(.))
tmp %>%
as.data.frame() %>%
mutate(var = rownames(.))
tmp %>%
as.data.frame() %>%
mutate(var = rownames(.)) %>%
select(var, MeanDecreaseGini)
tmp %>%
as.data.frame() %>%
mutate(var = rownames(.)) %>%
as_tibble()
tmp %>%
as.data.frame() %>%
mutate(var = rownames(.)) %>%
as_tibble() %>%
select(var, MeanDecreaseGini)
tmp %>%
as.data.frame() %>%
mutate(var = rownames(.)) %>%
as_tibble() %>%
select(2, 1)
tmp %>%
as.data.frame() %>%
mutate(var = rownames(.)) %>%
as_tibble() %>% colnames()
tmp %>%
as.data.frame() %>%
mutate(var = rownames(.)) %>%
as_tibble() %>%
select(var, everything())
tmp %>%
as.data.frame() %>%
mutate(var = rownames(.)) %>%
as_tibble()
tmp %>%
as.data.frame() %>%
mutate(var = rownames(.)) %>%
as_tibble() %>% colnames()
tmp %>%
as.data.frame() %>%
mutate(var = rownames(.)) %>%
as_tibble() %>%
select(var, MeanDecreaseGini)
tmp %>%
as.data.frame() %>%
mutate(var = rownames(.)) %>%
as_tibble() -> tmp.1
tmp.1[, c(2, 1)]
colnames(tmp.1) <- c("mean_gini_desc", "var")
tmp.1
tmp.1 %>%
select(var, mean_gini_desc)
library(tidyverse)
